{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally from https://skettee.github.io/post/q_network/ (in Korean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:12:55.202216Z",
     "start_time": "2019-10-30T07:12:54.836224Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:12:56.909881Z",
     "start_time": "2019-10-30T07:12:55.204317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output, Pretty\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:12:56.965149Z",
     "start_time": "2019-10-30T07:12:56.912537Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "N_STEP = 100\n",
    "N_EPISODE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:13:10.620201Z",
     "start_time": "2019-10-30T07:12:56.967571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 14 steps\n",
      "Episode finished after 9 steps\n",
      "Episode finished after 13 steps\n",
      "Episode finished after 10 steps\n",
      "Episode finished after 16 steps\n",
      "Episode finished after 15 steps\n",
      "Episode finished after 20 steps\n",
      "Episode finished after 21 steps\n",
      "Episode finished after 38 steps\n",
      "Episode finished after 34 steps\n",
      "State space:  Box(4,)\n",
      "Initial state:  [-0.03022219 -0.01513947  0.01063969  0.02018719]\n",
      "\n",
      "Action space:  Discrete(2)\n",
      "Random action:  1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "for i_episode in range(N_EPISODE):\n",
    "    observation = env.reset()\n",
    "    for t in range(N_STEP):\n",
    "        env.render()\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Episode finished after {} steps'.format(t + 1))\n",
    "            sleep(1)\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print('State space: ', env.observation_space)\n",
    "print('Initial state: ', state)\n",
    "print('\\nAction space: ', env.action_space)\n",
    "print('Random action: ', action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T01:47:03.065051Z",
     "start_time": "2019-10-29T01:46:30.215723Z"
    }
   },
   "source": [
    "### Action\n",
    "\n",
    "$A = \\{0, 1\\}$   \n",
    "\n",
    "Num\t| Action\n",
    "----|----\n",
    "0 |\tPush cart to the left\n",
    "1 |\tPush cart to the right\n",
    "\n",
    "\n",
    "### State\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "\\{ s_{00}, s_{01}, \\cdots\\}, \\\\\n",
    "\\{ s_{10}, s_{11}, \\cdots \\}, \\\\\n",
    "\\{ s_{20}, s_{21}, \\cdots \\}, \\\\\n",
    "\\{ s_{30}, s_{31}, \\cdots \\}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "Index | State                   | Min      | Max\n",
    "-----|-----------------|--------|------\n",
    "0      | Cart Position       | -4.8    | 4.8  \n",
    "1       | Cart Velocity       | -Inf     | Inf\n",
    "2      | Pole Angle           | -24 deg | 24 deg\n",
    "3      | Pole Velocity At Tip | -Inf     | Inf\n",
    "\n",
    "\n",
    "### Reward\n",
    "\n",
    "+1 for each step including the terminal step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X = \\{ x_1^{(i)}, x_2^{(i)}, x_3^{(i)}, x_4^{(i)} \\}$\n",
    "\n",
    "$Y = \\{ q_1^{(i)}, q_2^{(i)} \\}$\n",
    "\n",
    "$\\begin{align}\n",
    "J(w) & = \\mathbb E_{\\pi} [(q - \\hat q)^2] \\\\\n",
    "& =  \\mathbb E_{\\pi} \\left [\\left (R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t, w) \\right)^2 \\right ]\n",
    "\\end{align}$\n",
    "\n",
    "$ w \\leftarrow w-\\alpha \\dfrac{\\partial J(w,b)}{\\partial w}$  \n",
    "\n",
    "$\\hat A_t = \\text{argmax}_{a'}Q(S_t, a', w)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:13:10.756773Z",
     "start_time": "2019-10-30T07:13:10.622282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Network Modeling\n",
    "num_state = env.observation_space.shape[0]\n",
    "num_action = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim= num_state, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_action, activation=None))\n",
    "model.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:13:10.809900Z",
     "start_time": "2019-10-30T07:13:10.758610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:27:13.258022Z",
     "start_time": "2019-10-30T07:13:10.811960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc31259d20bf472cb0704a468d4038bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 00:13:10.926017 4639802816 deprecation_wrapper.py:119] From /Users/jeong/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_iteration = 500\n",
    "min_timesteps_per_batch = 2500\n",
    "\n",
    "# Hyper parameter\n",
    "epsilon = 0.3\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "\n",
    "# Q-Network Learning\n",
    "for i in tqdm(range(num_iteration)):\n",
    "    timesteps_this_batch = 0\n",
    "    memory = []\n",
    "    while True:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_value = model.predict(state.reshape(1, num_state))\n",
    "                action = np.argmax(q_value[0])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Memory\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        timesteps_this_batch += len(memory)\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "\n",
    "    # Replay   \n",
    "    for state, action, reward, next_state, done in memory:\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * (np.max(model.predict(next_state.reshape(1, num_state))[0]))\n",
    "        q_value = model.predict(state.reshape(1, num_state))\n",
    "        q_value[0][action] = target\n",
    "        model.fit(state.reshape(1, num_state), q_value, epochs=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:27:13.427690Z",
     "start_time": "2019-10-30T07:27:13.260337Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = os.getcwd()\n",
    "model_name = 'keras_dqn_trained_model.h5'\n",
    "\n",
    "# Save model and weights\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T07:28:12.781370Z",
     "start_time": "2019-10-30T07:27:13.429607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 279 steps\n",
      "Episode finished after 269 steps\n",
      "Episode finished after 277 steps\n",
      "Episode finished after 311 steps\n",
      "Episode finished after 287 steps\n",
      "Episode finished after 283 steps\n",
      "Episode finished after 263 steps\n",
      "Episode finished after 375 steps\n",
      "Episode finished after 285 steps\n",
      "Episode finished after 305 steps\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_path)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "num_state = env.observation_space.shape[0]\n",
    "\n",
    "for i_episode in range(N_EPISODE):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        state = np.array(state).reshape(1, num_state)\n",
    "        q_value = model.predict(state)\n",
    "        action = np.argmax(q_value[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} steps'.format(t + 1))\n",
    "            sleep(1)\n",
    "            break\n",
    "        t += 1      \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
