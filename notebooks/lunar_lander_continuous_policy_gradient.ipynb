{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally from https://skettee.github.io/post/policy_gradient/ (in Korean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:05:17.955159Z",
     "start_time": "2019-10-30T03:05:17.509217Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:05:17.996909Z",
     "start_time": "2019-10-30T03:05:17.957166Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output, Pretty\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:05:18.525833Z",
     "start_time": "2019-10-30T03:05:18.506445Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'LunarLanderContinuous-v2'\n",
    "N_STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:05:27.042112Z",
     "start_time": "2019-10-30T03:05:26.979566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box(8,)\n",
      "Initial state:  [-0.00424175  1.4180917  -0.42966294  0.31872183  0.00492195  0.09732513\n",
      "  0.          0.        ]\n",
      "\n",
      "Action space:  Box(2,)\n",
      "Random action:  [-0.9385834   0.45807928]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "print('State space: ', env.observation_space)\n",
    "print('Initial state: ', state)\n",
    "print('\\nAction space: ', env.action_space)\n",
    "print('Random action: ', action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T01:47:03.065051Z",
     "start_time": "2019-10-29T01:46:30.215723Z"
    }
   },
   "source": [
    "### Action\n",
    "\n",
    "Each action is defined as 2 real numbers between -1 and 1.\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "\\{ a_{00}, a_{01} \\}, \\\\\n",
    "\\{ a_{10}, a_{11} \\}, \\\\\n",
    "\\{ a_{20}, a_{21} \\}, \\\\\n",
    "\\{ a_{30}, a_{31} \\}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "Index | State                   | Control\n",
    "------|-----------------|-------------\n",
    "0     | Main engine       | -1..0 off, 0..+1 throttle from 50% to 100% power\n",
    "1     | Left or right engine | -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "\n",
    "\n",
    "### State\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "\\{ s_{00}, s_{01}, \\cdots\\}, \\\\\n",
    "\\{ s_{10}, s_{11}, \\cdots \\}, \\\\\n",
    "\\{ s_{20}, s_{21}, \\cdots \\}, \\\\\n",
    "\\{ s_{30}, s_{31}, \\cdots \\}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "Index | State                                           | Value\n",
    "-----|---------------------------------|-----------\n",
    "0      | X position               | -Inf ~ Inf\n",
    "1      | Y position               | -Inf ~ Inf\n",
    "2      | X velocity                                | -Inf ~ Inf\n",
    "3      | Y velocity                                | -Inf ~ Inf\n",
    "4      | Angle                  | -Inf ~ Inf\n",
    "5      | Angular velocity | -Inf ~ Inf\n",
    "6      | If the left lag touches the ground           | 1 (Yes), 0 (No)\n",
    "7      | If the right lag touches the ground         | 1 (Yes), 0 (No)\n",
    "\n",
    "\n",
    "### Reward\n",
    "\n",
    "- Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
    "- If lander moves away from landing pad it loses reward back.\n",
    "- Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.\n",
    "- Each leg ground contact is +10\n",
    "- Firing main engine is -0.3 points each frame\n",
    "- Firing side engine is -0.03 points each frame\n",
    "- Solved is 200 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X = \\{ s_1^{(i)}, s_2^{(i)}, s_3^{(i)}, s_4^{(i)},  s_5^{(i)}, s_6^{(i)}, s_7^{(i)}, s_8^{(i)} \\}$\n",
    "\n",
    "$Y = \\{a_1^{(i)}, a_2^{(i)})\\}$\n",
    "\n",
    "$\\begin{align}\n",
    "& \\hat Y = \\{ \\hat a_1^{(i)}, \\hat a_2^{(i)} \\} \\\\\n",
    "& \\hat a_1^{(i)} = \\hat \\mu_1 (s, \\theta) \\\\\n",
    "& \\hat a_2^{(i)} = \\hat \\mu_2 (s, \\theta)\n",
    "\\end{align}$\n",
    "\n",
    "$J(\\theta) = - \\dfrac{1}{N} \\sum_{i=1}^N  \\nabla_{\\theta} \\log p(a ; \\theta) * \\hat A_{\\text{std}}$\n",
    "\n",
    "\n",
    "$\\theta \\leftarrow \\theta-\\alpha \\dfrac{\\partial J(\\theta)}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:23:46.072455Z",
     "start_time": "2019-10-30T03:23:44.096716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 20:23:45.812487 4598023616 deprecation.py:506] From /Users/jeong/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Input Dimensions\n",
    "states_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Placeholders for inputs.\n",
    "input_state = tf.placeholder(shape=[None, states_dim], name=\"state\", dtype=tf.float32)\n",
    "input_action = tf.placeholder(shape=[None, action_dim], name=\"action\", dtype=tf.float32)\n",
    "input_advan = tf.placeholder(shape=[None], name=\"advan\", dtype=tf.float32)\n",
    "n = tf.shape(input_state)[0]\n",
    "\n",
    "# The log std vector, it's a parameter.\n",
    "logstd = tf.get_variable(\"logstd\", shape=[action_dim], initializer=tf.zeros_initializer())\n",
    "logstd_n = tf.ones(shape=(n, action_dim), dtype=tf.float32) * logstd\n",
    "\n",
    "# The policy network.\n",
    "hidden1 = Dense(32, activation='relu')(input_state)\n",
    "hidden2 = Dense(32, activation='relu')(hidden1)\n",
    "mean = Dense(action_dim, activation=None)(hidden2)\n",
    "\n",
    "# Computing the log likelihood. \n",
    "variance = tf.exp(2*logstd_n)\n",
    "func = -tf.square(input_action - mean)/(2*variance) - 0.5*tf.log(tf.constant(2*np.pi)) - logstd_n\n",
    "log_likelihood = tf.reduce_sum(func, axis=[1])\n",
    "\n",
    "# Diagonal Gaussian distribution for sampling actions\n",
    "sampled_action = (tf.random_normal(tf.shape(mean)) * tf.exp(logstd_n) + mean)[0]\n",
    "\n",
    "# Loss function\n",
    "loss_op = - tf.reduce_mean(log_likelihood * input_advan) \n",
    "# Optimizer\n",
    "update = tf.train.AdamOptimizer().minimize(loss_op) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T18:44:05.692403Z",
     "start_time": "2019-10-29T18:44:05.648185Z"
    }
   },
   "source": [
    "## State-Value Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X = \\{ s_1^{(i)}, s_2^{(i)}, s_3^{(i)}, s_4^{(i)},  s_5^{(i)}, s_6^{(i)}, s_7^{(i)}, s_8^{(i)}\\}$\n",
    "\n",
    "$Y = v^{(i)} = G^{(i)}_t$\n",
    "\n",
    "$\\hat Y = \\hat v^{(i)} =  w^T X + b $\n",
    "\n",
    "${\\large J}(w, b) = \\dfrac{1}{2m} \\sum_{i=1}^m (G_t^{(i)} - \\hat v^{(i)} (S, w, b))^2$  \n",
    "\n",
    "$\\begin{align}\n",
    "w & \\leftarrow w-\\alpha \\dfrac{\\partial J(w,b)}{\\partial w} \\\\    \n",
    "b & \\leftarrow b-\\alpha \\dfrac{\\partial J(w,b)}{\\partial b}\n",
    "\\end{align}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:28:22.559409Z",
     "start_time": "2019-10-30T03:28:21.869654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# State-value function approximator\n",
    "approx = LinearRegression()\n",
    "\n",
    "# Initialization\n",
    "approx.fit(np.array([0]*states_dim).reshape((1,states_dim)), np.array([0]).reshape((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T03:29:32.686513Z",
     "start_time": "2019-10-30T03:29:32.579845Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def get_returns(x, gamma):\n",
    "    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "\n",
    "def predict(sess, state):\n",
    "    with sess.as_default():\n",
    "        action = sess.run(sampled_action, feed_dict={input_state: state[None]})\n",
    "    return action\n",
    "\n",
    "def fit(sess, state, action, advantage):\n",
    "    with sess.as_default():\n",
    "        sess.run([update], feed_dict={input_state: state, input_action: action, input_advan: advantage})      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:04:07.946408Z",
     "start_time": "2019-10-30T03:29:47.092090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [34:20<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_iteration = 500\n",
    "gamma = 0.97\n",
    "min_timesteps_per_batch = 3000\n",
    "total_timesteps = 0\n",
    "\n",
    "for i in tqdm(range(num_iteration)):\n",
    "    timesteps_this_batch = 0\n",
    "    memory = []\n",
    "    while True:\n",
    "        state = env.reset()\n",
    "        states, actions, rewards = [], [], []\n",
    "        while True:\n",
    "            states.append(state)\n",
    "            action = predict(sess, state)\n",
    "            actions.append(action)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Memory\n",
    "        path = {\"state\" : np.array(states), \"action\" : np.array(actions), \"reward\" : np.array(rewards)}\n",
    "        memory.append(path)        \n",
    "        timesteps_this_batch += len(path[\"reward\"])\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    total_timesteps += timesteps_this_batch\n",
    "    \n",
    "    # Replay\n",
    "    \n",
    "    # Advantage function\n",
    "    G_ts, advs = [], []\n",
    "    for path in memory:\n",
    "        rew_t = path[\"reward\"]\n",
    "        G_t = get_returns(rew_t, gamma)\n",
    "        v_hat = approx.predict(path[\"state\"])\n",
    "        adv = G_t - v_hat.flatten()\n",
    "        advs.append(adv)\n",
    "        G_ts.append(G_t)\n",
    "   \n",
    "    # Build data set\n",
    "    in_states = np.concatenate([path[\"state\"] for path in memory])\n",
    "    in_actions  = np.concatenate([path[\"action\"] for path in memory])\n",
    "    advans = np.concatenate(advs)\n",
    "    in_advans = (advans - advans.mean()) / (advans.std() + 1e-8)    \n",
    "    G_ts = np.concatenate(G_ts)\n",
    "    # Update State-value\n",
    "    approx.fit(in_states, G_ts)\n",
    "    # Update Policy\n",
    "    fit(sess, in_states, in_actions, in_advans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T05:26:54.528443Z",
     "start_time": "2019-10-30T05:26:52.065358Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    action = predict(sess, state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
